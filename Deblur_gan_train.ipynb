{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Only for colab\n",
    "!git clone https://github.com/hassansadiq1/Deblur_GAN.git\n",
    "import os\n",
    "os.chdir(\"Deblur_GAN/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, cv2, os\n",
    "\n",
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "#from deblurgan.utils import write_log\n",
    "from losses import wasserstein_loss, perceptual_loss\n",
    "from model import generator_model, discriminator_model, generator_containing_discriminator_multiple_outputs\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "BASE_DIR = 'weights/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters for our model\n",
    "input_shape = (256,256,3)\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "input_directory = \"/home/hani/hassan/git_repos/deblur-gan/gopro\"\n",
    "log_directory = \"/home/hani/hassan/git_repos/deblur-gan\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read images from input directory\n",
    "def load_images(images_path):\n",
    "    X_images = []\n",
    "    for img in images_path:\n",
    "      X_images.append(cv2.resize(cv2.imread(img), input_shape[:-1]))\n",
    "    X_images = np.array(X_images)\n",
    "    #Preprocessing i.e normalizing data\n",
    "    X_images = (X_images - 127.5) / 127.5\n",
    "    return X_images\n",
    "\n",
    "def save_all_weights(d, g, epoch_number, current_loss):\n",
    "    now = datetime.datetime.now()\n",
    "    save_dir = os.path.join(BASE_DIR, '{}{}'.format(now.month, now.day))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    g.save_weights(os.path.join(save_dir, 'generator_{}_{}.h5'.format(epoch_number, current_loss)), True)\n",
    "    d.save_weights(os.path.join(save_dir, 'discriminator_{}.h5'.format(epoch_number)), True)\n",
    "\n",
    "def train_multiple_outputs(images_path, batch_size, log_dir, epoch_num, critic_updates=5):\n",
    "    x_path = glob.glob(images_path + \"/A/*\")\n",
    "    y_path = glob.glob(images_path + \"/B/*\")\n",
    "    x_train = load_images(x_path)\n",
    "    y_train = load_images(y_path)\n",
    "\n",
    "    g = generator_model(input_shape)\n",
    "    d = discriminator_model(input_shape)\n",
    "    d_on_g = generator_containing_discriminator_multiple_outputs(g, d, input_shape)\n",
    "\n",
    "    d_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    d_on_g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    d.trainable = True\n",
    "    d.compile(optimizer=d_opt, loss=wasserstein_loss)\n",
    "    d.trainable = False\n",
    "    loss = [perceptual_loss, wasserstein_loss]\n",
    "    loss_weights = [100, 1]\n",
    "    d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
    "    d.trainable = True\n",
    "\n",
    "    output_true_batch, output_false_batch = np.ones((batch_size, 1)), -np.ones((batch_size, 1))\n",
    "\n",
    "    log_path = './logs'\n",
    "    tensorboard_callback = TensorBoard(log_path)\n",
    "    print(\"returning\")\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(epoch_num)):\n",
    "        permutated_indexes = np.random.permutation(x_train.shape[0])\n",
    "\n",
    "        d_losses = []\n",
    "        d_on_g_losses = []\n",
    "        for index in range(int(x_train.shape[0] / batch_size)):\n",
    "            batch_indexes = permutated_indexes[index*batch_size:(index+1)*batch_size]\n",
    "            image_blur_batch = x_train[batch_indexes]\n",
    "            image_full_batch = y_train[batch_indexes]\n",
    "\n",
    "            generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n",
    "\n",
    "            for _ in range(critic_updates):\n",
    "                d_loss_real = d.train_on_batch(image_full_batch, output_true_batch)\n",
    "                d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "                d_losses.append(d_loss)\n",
    "\n",
    "            d.trainable = False\n",
    "\n",
    "            d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [image_full_batch, output_true_batch])\n",
    "            d_on_g_losses.append(d_on_g_loss)\n",
    "\n",
    "            d.trainable = True\n",
    "\n",
    "        # write_log(tensorboard_callback, ['g_loss', 'd_on_g_loss'], [np.mean(d_losses), np.mean(d_on_g_losses)], epoch_num)\n",
    "        print(np.mean(d_losses), np.mean(d_on_g_losses))\n",
    "        with open('log.txt', 'a+') as f:\n",
    "            f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n",
    "\n",
    "        save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning\n",
      "WARNING:tensorflow:From /home/hani/hassan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "-0.4606336207718414 1677.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [21:04<00:00, 1264.21s/it]\n"
     ]
    }
   ],
   "source": [
    "train_multiple_outputs(input_directory, batch_size, log_directory, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npy_chunk(filename, start_row, num_rows):\n",
    "    assert start_row >= 0 and num_rows > 0\n",
    "    with open(filename, 'rb') as fhandle:\n",
    "        major, minor = np.lib.format.read_magic(fhandle)\n",
    "        shape, fortran, dtype = np.lib.format.read_array_header_1_0(fhandle)\n",
    "        assert not fortran, \"Fortran order arrays not supported\"\n",
    "        # Make sure the offsets aren't invalid.\n",
    "        assert start_row < shape[0], (\n",
    "            'start_row is beyond end of file'\n",
    "        )\n",
    "        assert start_row + num_rows <= shape[0], (\n",
    "            'start_row + num_rows > shape[0]'\n",
    "        )\n",
    "        # Get the number of elements in one 'row' by taking\n",
    "        # a product over all other dimensions.\n",
    "        row_size = np.prod(shape[1:])\n",
    "        start_byte = start_row * row_size * dtype.itemsize\n",
    "        fhandle.seek(start_byte, 1)\n",
    "        n_items = row_size * num_rows\n",
    "        flat = np.fromfile(fhandle, count=n_items, dtype=dtype)\n",
    "        return flat.reshape((-1,) + shape[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hassan",
   "language": "python",
   "name": "hassan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
